[{"image":"https://github.com/openproblems-bio/openproblems/pkgs/container/openproblems","task_id":"label_projection","maximize": true,"metric_id":"accuracy","commit_sha":"b3456fd73c04c28516f6df34c57e6e3e8b0dab32","references":"grandini2020metrics","metric_name":"Accuracy","code_version":"v1.0.0","metric_summary":"Average number of correctly applied labels.","paper_reference":"grandini2020metrics","implementation_url":"https://github.com/openproblems-bio/openproblems/blob/v1.0.0/openproblems/tasks/label_projection/metrics/accuracy.py"},{"image":"https://github.com/openproblems-bio/openproblems/pkgs/container/openproblems","task_id":"label_projection","maximize": true,"metric_id":"f1","commit_sha":"b3456fd73c04c28516f6df34c57e6e3e8b0dab32","references":"grandini2020metrics","metric_name":"F1 score","code_version":"v1.0.0","metric_summary":"The [F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) is a weighted average of the precision and recall over all class labels, where an F1 score reaches its best value at 1 and worst score at 0, where each class contributes to the score relative to its frequency in the dataset.","paper_reference":"grandini2020metrics","implementation_url":"https://github.com/openproblems-bio/openproblems/blob/v1.0.0/openproblems/tasks/label_projection/metrics/f1.py"},{"image":"https://github.com/openproblems-bio/openproblems/pkgs/container/openproblems","task_id":"label_projection","maximize": true,"metric_id":"f1_macro","commit_sha":"b3456fd73c04c28516f6df34c57e6e3e8b0dab32","references":"grandini2020metrics","metric_name":"Macro F1 score","code_version":"v1.0.0","metric_summary":"The macro F1 score is an unweighted F1 score, where each class contributes equally, regardless of its frequency.","paper_reference":"grandini2020metrics","implementation_url":"https://github.com/openproblems-bio/openproblems/blob/v1.0.0/openproblems/tasks/label_projection/metrics/f1.py"}]